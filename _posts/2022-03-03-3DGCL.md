---
layout: single
title: "3D Graph Contrastive Learning for Molecular Property Prediction"
---

# Abstract
SSL은 unlabeled data를 활용한 supervision을 통해 data의 representation을 학습하는 방법으로, 다른 분야에 비해 labeled data가 부족하고 labeling에 비용과 시간이 크게 소요되는 drug 분야에서 특히 각광받고 있다. 방대한 unlabeled data를 사용한 SSL for molecular representation들은 뛰어난 성능을 보이고 있지만, 여기에는 몇 가지 이슈가 존재한다. (1) 기존의 SSL 모델은 'large-scale'이다. 다양한 downstream task에 대한 일반화를 위해 엄청난 양의 pretrain data가 필요하고 이를 학습하기 위해 transformer와 같이 사이즈가 큰 모델을 사용하는 경우가 많다. 따라서, 자원이 제한된 환경에서 SSL을 구현하는 데는 한계가 존재한다. (2) 약물의 3D 구조 정보를 활용하지 않는다. 약물의 작용은 약물 분자의 구조와 밀접한 관련이 있다. 하지만, 대부분의 SSL 모델들이 3D 정보를 활용하지 않거나, 부분적으로 활용하고 있다. (3) molecule에 contrastive learning을 활용하는 기존의 모델들은 atom이나 bond를 변경하는 augmentation 방법을 사용한다. 약물 분자 표현은 이미지와 달리 단순 변환이라도 완전히 다른 약물 특성을 초래할 수 있기 때문에 이와 같은 방법을 사용한다면 서로 다른 분자들이 positive sample로 구성될 수 있다. 위의 문제들을 해결하기 위해, 본 연구에서는 새로운 small-scale의 **3D** **G**raph **C**ontrastive **L**earning for Molecular Property Prediction (*3DGCL*) 을 제안한다. 3DGCL은 약물의 성질을 변경하지 않는 3D pretext task를 통해 molecule의 구조를 반영하여 representation을 학습한다. 1,128개의 pretrain dataset과 1 million 이하의 model parameter만을 사용하여 4가지 benchmark dataset에서 SOTA 혹은 comparable performance을 달성하였고, 기존의 방법들 보다 pretraining의 효과가 가장 뛰어났다. 실험은 molecular property prediction을 위한 molecular representation learning에 3D 구조 정보가 필수적이며, SSL을 위해 large-scale에 집중하는 것 뿐만 아니라 정확한 표현 학습을 위한 pretext task에 집중해야함을 보여준다.

<br />
<br />

# Introduction
## 기존 방법의 문제점
1. SSL시 molecule의 구조를 변경함. 분자는 이미지와 달리 단순 변환이라도 완전히 다른 특성을 초래할 수 있음.
2. 매우 많은 pretrain 데이터 사용
3. 매우 많은 모델 파라미터 사용
4. 3D 구조를 반영하지 못함
<br />

## Novelty
1. molecule의 성질을 유지시키면서 contrastive learning 하는 새로운 방법 (conformer, rotation, noise)
2. pretrain data로 1128개만 사용
3. 적은 파라미터 사용 (< 1 million, qm8 기준 2시간 30분 소요)
4. 3D 구조 활용

<br />

# Related Work
1. 3DGNN
2. SSL
3. Graph Contrastive

<br />

# Methods

1. Conformer
2. Noise
3. Drop Atom
4. Atom masking
<br />

# Experiments
- 3 scaffold + 3 times
- dropout 0.0, 0.3만 비교할 것

## Freesolv
### **Pretraining setup**
#### 1. pretrain의 MMFF 비교 - 1,4 > 3,4 **비교중**
#### 2. pretrain의 gamma 비교 - 0.99 > 0.97 > 0.95
#### 3. pretrain의 MLP 비교 -  ** 비교중**

### **Fine-tuning setup**
#### 1. finetune의 gamma 비교 - 0.99 > 0.97 > 0.95
#### 2. finetune의 weight decay 비교 - 1e-4 > 0 ** 비교중**

<br />

## ESOL
### **Pretraining setup**
#### 1. pretrain의 MMFF 비교 - 1,2 > 3,4 > 1,4
#### 2. pretrain의 gamma 비교 - 0.95 > 0.97 > 0.99
#### 3. pretrain의 MLP 비교 -  

### **Fine-tuning setup**
#### 1. finetune의 gamma 비교 - 0.95 > 0.97 > 0.99
#### 2. finetune의 weight decay 비교 - 1e-3, 1e-5  ** 비교중**

<br />


# Results
- supervised 보다 pretrain 했을 경우 성능이 향상되는 정도가 가장 뛰어남. 우리의 방법이 3D 분자 표현을 학습하는데에 적합한 방법임.
- dataset으로 small molecule (<20) + low level (Regression) 4가지 dataset 사용
<br />

|Dataset  |ESOL   |Freesolv| QM7   | QM8    |Pretrain| Params | Improvement |
|---      |---    |---     | ---   | ---    | ---    | ---    |--- |
|CoMPT    | 0.798 | 1.855  | ---   | ---    | ---    | ---    |--- |
|No GROVER| 0.911 | 1.987  | 89.408| 0.017  | ---    | ---    |--- |
|GROVER   | 0.831 | 1.544  | 72.5  |**0.0125**| 11 MM|100 MM| 15.55 / 19.15  |
|No MPG   | 0.896 | 1.967  | ---   | ---    | ---    | ---    |--- |
|MPG      |**0.741**|**1.269**|--- | ---    | 11 MM  | 50 MM  | 26.75 / -- |
|No GEM   | 0.832 | 1.857  | 59.0  | 0.0173 | ---    | ---    |--- |
|GEM      | 0.798 | 1.877  | 58.9  | 0.0171 | 20 MM  | ?? MM  | 14.1 / -0.1 / 0.02 / 1.2  |
|No 3DGCL | 1.012 | 2.379  | 50.8  | 0.0175 | ---    | ---    | --- |
|3DGCL    |**0.722**| 1.533  |**43.4**| 0.0126| 1,128  |1MM     | **31 / 26.15** |

<br />

![pretrain](../images/2022-01-29-3DGCL/pretrain.png)

<br />

![params](../images/2022-01-29-3DGCL/params.png)

<br />

|Dataset  |ESOL     |Freesolv | QM7     | QM8    |  Pretrain  |
|---      |---      |---      | ---     | ---    |     ---    |
|GROVER   | 0.831   | 1.544   | 72.5    | 0.0125 | 11 millions|
|MPG      |0.741|**1.269**|---      | ---    | 11 millions|
|GEM      |0.798    |1.877    |58.9     | 0.0171 | 20 millions|
|3DGCL    | **0.733**   | 1.533   | **43.4**| 0.0126 | 1,128      |

<br />


|Dataset  |ESOL   |Freesolv| QM7   | QM8    |  
|---      |---    |---     | ---   | ---    |
|pretrain | ESOL  | ESOL   | ESOL  | ESOL   |
|cutoff   | 5.0   | 8.0    | 8.0   | 8.0    |
|layers   | 2     | 2      | 2     | 2      | 
|filters  | 128   | 512    | 256   | 256    |
|gaussians| 50    | 50     | 100   | 100    |
|z_dim    | 512   | 256    | 128   | 128    | 
|dropout  | 0.3   | 0.1    | 0.2   | ---    | 
|optim    | Expo  | Expo   | Expo  | Expo   | 
|Super    | 1.012 | 2.379  | 50.8  | 0.0175 | 
|Pretrain | 0.745 | 1.533  | 43.4  | 0.0126 | 
|SOTA     | Yes   | Yes    | Yes   | No     | 
|Decrease | 26.4% | 35.6%  | 14.6% | 28%    | 

<br />

## ESOL
|Augmentation|ETKDG|Noise|Atom Dropping|Attribute Masking|
|---|---|---|---|---|
|ETKDG             | 0.867 | 0.867 | 0.867 | 0.867 |
|Noise             | 0.867 | 0.867 | 0.867 | 0.867 |
|Atom Dropping     | 0.867 | 0.867 | 0.867 | 0.867 |
|Attribute Masking | 0.867 | 0.867 | 0.867 | 0.867 |

<br />
