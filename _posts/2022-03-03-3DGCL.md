---
layout: single
title: "3D Graph Contrastive Learning for Molecular Property Prediction"
---

# Abstract
 Self-supervised learning is a method that learns the data representation through supervision using unlabeled data, attracting attention, especially in the drug field, which lacks labeled data and spends more time and money than other fields.
 **SSL + Graph 설명 부족 / 추가**
SSL은 unlabeled data를 활용한 supervision을 통해 data의 representation을 학습하는 방법으로, 
Self-supervised learning is a method that learns the data representation through supervision using unlabeled data.

다른 분야에 비해 labeled data가 부족하고 labeling에 비용과 시간이 크게 소요되는 drug 분야에서 특히 각광받고 있다.
This learning method is in the spotlight, especially in the drug field, which lacks labeled data and spends more time and money than other fields.

방대한 unlabeled data를 사용한 SSL for molecular representation들은 뛰어난 성능을 보이고 있지만, 여기에는 몇 가지 이슈가 존재한다. 
Self-supervised learning using enormous unlabeled data has shown excellent performance for molecular property prediction, but there are a few issues.

(1) 기존의 SSL 모델은 'large-scale'이다. 다양한 downstream task에 대한 일반화를 위해 엄청난 양의 pretrain data가 필요하고 이를 학습하기 위해 transformer와 같이 사이즈가 큰 모델을 사용하는 경우가 많다. 
Existing models require vast millions of pre-train data to generalize various downstream tasks, and in many cases, are large-size models such as transformer to learn that data.

따라서, 자원이 제한된 환경에서 SSL을 구현하는 데는 한계가 존재한다. 
So, there is a limitation to implementing SSL where the computing resource is not enough.

(2) 약물의 3D 구조 정보를 활용하지 않는다. 
 In most cases, they do not utilize 3D structural information for molecular representation learning.
 약물의 작용은 약물 분자의 구조와 밀접한 관련이 있다. 
The activity of a drug is closely related to the structure of the drug molecule.
 
하지만, 대부분의 SSL 모델들이 3D 정보를 활용하지 않거나, 부분적으로 활용하고 있다.
But, most current models do not use 3D information or use it partially.

(3) Contrastive learning is one of the self-supervised learning methods and consists of pre-text tasks learning from positive and negative samples. **contrastive 설명 추가 / 부족**

molecule에 contrastive learning을 활용하는 기존의 모델들은 atom이나 bond를 변경하는 augmentation 방법을 사용한다. 
Previous models which apply contrastive learning to molecules make use of the augmentation permuting atoms and bonds.

약물 분자 표현은 이미지와 달리 단순 변환이라도 완전히 다른 약물 특성을 초래할 수 있기 때문에 이와 같은 방법을 사용한다면 서로 다른 분자들이 positive sample로 구성될 수 있다. 
Unlike images, drugs can have completely different properties if we use the augmentation that changes atoms or bonds, so molecules having different characteristics can be in the same positive samples.

위의 문제들을 해결하기 위해, 본 연구에서는 새로운 small-scale의 **3D** **G**raph **C**ontrastive **L**earning for Molecular Property Prediction (*3DGCL*) 을 제안한다. 
To solve the above problems, we propose a novel contrastive learning framework, small-scale 3D Graph Contrastive Learning for molecular property prediction (*3DGCL*).
**표절**

3DGCL은 약물의 성질을 변경하지 않는 3D pretext task를 통해 molecule의 구조를 반영하여 representation을 학습한다. 
3DGCL learns the molecular representation by reflecting the structure of the molecule through the 3D task that does not change the properties of the drug.

1,128개의 pretrain dataset과 1 million 이하의 model parameter만을 사용하여 4가지 benchmark dataset에서 SOTA 혹은 comparable performance을 달성하였고, 기존의 방법들 보다 pretraining의 효과가 가장 뛰어났다. 
Using only 1,128 pre-train datasets and 1 million model parameters, we achieved the state-of-the-art or comparable performance and the best result in the pre-training effect than existing methods.

실험은 molecular property prediction을 위한 molecular representation learning에 3D 구조 정보가 필수적이며, SSL을 위해 large-scale에 집중하는 것 뿐만 아니라 정확한 표현 학습을 위한 pretext task에 집중해야함을 보여준다.
The results show that 3D structural information is essential to molecular representation learning for molecular property prediction and that we should focus on not only large-scale but also the pre-text task to learn representations properly in self-supervised learning.

<br />
<br />

# Introduction
## 기존 방법의 문제점
1. SSL시 molecule의 구조를 변경함. 분자는 이미지와 달리 단순 변환이라도 완전히 다른 특성을 초래할 수 있음.
2. 매우 많은 pretrain 데이터 사용
3. 매우 많은 모델 파라미터 사용
4. 3D 구조를 반영하지 못함
<br />

## Novelty
1. molecule의 성질을 유지시키면서 contrastive learning 하는 새로운 방법 (conformer, rotation, noise)
2. pretrain data로 1128개만 사용
3. 적은 파라미터 사용 (< 1 million, qm8 기준 2시간 30분 소요)
4. 3D 구조 활용

<br />

# Related Work
1. 3DGNN
2. SSL
3. Graph Contrastive

<br />

# Methods

1. Conformer
2. Noise
3. Drop Atom
4. Atom masking
<br />

# Experiments
- 3 scaffold + 3 times
- dropout 0.0, 0.3만 비교할 것

## Freesolv
- Freesolv는  pretrain epoch, dropout 찜찜함..
- finetune의 dropout과 pretrain epoch 줄이는 것이 비슷한 효과를 나타내는 것으로 추정
### **Pretraining setup**
#### 1. pretrain의 MMFF 비교 - 1,4 > 3,4 / 1,2  **RAMDOM** **비교중**
#### 2. pretrain의 gamma 비교 - 0.99 > 0.97 > 0.95 (고정)
#### 3. pretrain의 MLP 비교 -  ReLU > SSP (고정)
#### 4. pretrain의 epoch 비교 -  39(1, 2) > 420, 432 **비교중**
#### 5. pretrain의 dropout 비교 - 0.2 > 0.0 **비교중**
MMFF, dropout, epoch 비교!!!  

### **Fine-tuning setup**
#### 1. finetune의 gamma 비교 - 0.99 > 0.97 > 0.95
#### 2. finetune의 weight decay 비교 - 1e-4 > 0 **비교중**
#### 3. finetune의 MLP 비교 -  SSP > ReLU
<br />

## ESOL
- ESOL은 pretrain이 best 일때 성능이 좋아보임
### **Pretraining setup**
#### 1. pretrain의 MMFF 비교 - 1,2 > 3,4 > 1,4 (고정)
#### 2. pretrain의 gamma 비교 - 0.95 > 0.97 > 0.99 (고정)
#### 3. pretrain의 MLP 비교 -  ReLU > SSP (고정)
#### 4. pretrain의 epoch 비교 -  375 > 30 > 10 (고정)
#### 5. pretrain의 dropout 비교 - 0.0 > 0.2 (고정)

### **Fine-tuning setup**
#### 1. finetune의 gamma 비교 - 0.95 > 0.97 > 0.99 (고정)
#### 2. finetune의 weight decay 비교 - 1e-6 > 5e-6 / 1e-4 = 5e-5 / 1e-3 > 1e-5
비교중 - 1e-6 vs 1e-4 vs 5e-5 vs 5e-4
#### 3. finetune의 MLP 비교 - SSP > ReLU (고정)


<br />


# Results
- supervised 보다 pretrain 했을 경우 성능이 향상되는 정도가 가장 뛰어남. 우리의 방법이 3D 분자 표현을 학습하는데에 적합한 방법임.
- dataset으로 small molecule (<20) + low level (Regression) 4가지 dataset 사용
<br />

|Dataset  |ESOL   |Freesolv| QM7   | QM8    |Pretrain| Params | Improvement |
|---      |---    |---     | ---   | ---    | ---    | ---    |--- |
|CoMPT    | 0.798 | 1.855  | ---   | ---    | ---    | ---    |--- |
|No GROVER| 0.911 | 1.987  | 89.408| 0.017  | ---    | ---    |--- |
|GROVER   | 0.831 | 1.544  | 72.5  |**0.0125**| 11 MM|100 MM| 15.55 / 19.15  |
|No MPG   | 0.896 | 1.967  | ---   | ---    | ---    | ---    |--- |
|MPG      |**0.741**|**1.269**|--- | ---    | 11 MM  | 50 MM  | 26.75 / -- |
|No GEM   | 0.832 | 1.857  | 59.0  | 0.0173 | ---    | ---    |--- |
|GEM      | 0.798 | 1.877  | 58.9  | 0.0171 | 20 MM  | ?? MM  | 14.1 / -0.1 / 0.02 / 1.2  |
|No 3DGCL | 1.012 | 2.379  | 50.8  | 0.0175 | ---    | ---    | --- |
|3DGCL    |**0.722**| 1.526  |**43.4**| 0.0126| 1,128  |1MM     | **31 / 26.15** |

<br />

![pretrain](../images/2022-01-29-3DGCL/pretrain.png)

<br />

![params](../images/2022-01-29-3DGCL/params.png)

<br />

|Dataset  |ESOL     |Freesolv | QM7     | QM8    |  Pretrain  |
|---      |---      |---      | ---     | ---    |     ---    |
|GROVER   | 0.831   | 1.544   | 72.5    | 0.0125 | 11 millions|
|MPG      |0.741|**1.269**|---      | ---    | 11 millions|
|GEM      |0.798    |1.877    |58.9     | 0.0171 | 20 millions|
|3DGCL    | **0.733**   | 1.533   | **43.4**| 0.0126 | 1,128      |

<br />


|Dataset  |ESOL   |Freesolv| QM7   | QM8    |  
|---      |---    |---     | ---   | ---    |
|pretrain | ESOL  | ESOL   | ESOL  | ESOL   |
|cutoff   | 5.0   | 8.0    | 8.0   | 8.0    |
|layers   | 2     | 2      | 2     | 2      | 
|filters  | 128   | 512    | 256   | 256    |
|gaussians| 50    | 50     | 100   | 100    |
|z_dim    | 512   | 256    | 128   | 128    | 
|dropout  | 0.3   | 0.1    | 0.2   | ---    | 
|optim    | Expo  | Expo   | Expo  | Expo   | 
|Super    | 1.012 | 2.379  | 50.8  | 0.0175 | 
|Pretrain | 0.745 | 1.533  | 43.4  | 0.0126 | 
|SOTA     | Yes   | Yes    | Yes   | No     | 
|Decrease | 26.4% | 35.6%  | 14.6% | 28%    | 

<br />

## ESOL
|Augmentation|ETKDG|Noise|Atom Dropping|Attribute Masking|
|---|---|---|---|---|
|ETKDG             | 0.867 | 0.867 | 0.867 | 0.867 |
|Noise             | 0.867 | 0.867 | 0.867 | 0.867 |
|Atom Dropping     | 0.867 | 0.867 | 0.867 | 0.867 |
|Attribute Masking | 0.867 | 0.867 | 0.867 | 0.867 |

<br />
