---
layout: single
title:  "실습 예제입니다."
---

```python
import sys
sys.path.insert(0,'..')
sys.path.insert(0,'../..')
import torch
#print(torch.version.cuda)
#print(torch.__version__)

from dig.sslgraph.utils import Encoder
from dig.sslgraph.evaluation import GraphUnsupervised
from dig.sslgraph.evaluation import Finetune
from dig.threedgraph.dataset import MoleculeNet
from dig.threedgraph.method import SphereNet, SchNet, DimeNetPP
from dig.sslgraph.method import GraphCL

import matplotlib.pyplot as plt

from rdkit import RDLogger 
RDLogger.DisableLog('rdApp.*')

from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))

import pandas as pd
from rdkit import Chem
import rdkit.Chem.AllChem as AllChem

import argparse

paser = argparse.ArgumentParser()
args = paser.parse_args("")
```


<style>.container { width:90% !important; }</style>


# Grid Search


```python
# Finetune or rand init
args.finetune = False
args.seed = 2222

# File Path
args.model_path = './models/enc_pretrain-moleculenet_batch-512_encoder-schnet_cutoff-5.0\
_num_layers-4_z_dim-512_lr-0.0005_aug_1-ETKDG1_aug_2-ETKDG2_aug_ratio-0.2_tau-0.2_optim-Cosine_dropout-0.4_T_0-20_T_mult-2_eta_max-0.01_T_up-10_gamma-0.5/enc_best_epoch-170_loss-688.899.pkl'

# Device
args.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# Dataset
args.pretrain_dataset = 'pubchem01'
args.dataset = 'lipo'
args.batch_size = 32

# Model
args.encoder = 'schnet'
args.cutoff = 5.0  # [5.0, 10.0]
args.num_layers = 4 # [2, 4]
args.z_dim = 512
args.num_filters = 128
args.num_gaussians = 50

# Learning
args.n_folds = 5
args.f_epoch = 150
args.f_lr = 5e-4
args.aug_1, args.aug_2 = 'ETKDG1', 'ETKDG2'
args.aug_ratio = 0.2
args.tau = 0.2
args.proj = 'schnet'

# Regularization
args.dropout_rate = 0.2

args.f_optim = 'StepLR' #['Cosine', 'StepLR']
args.f_weight_decay = 0
args.f_lr_decay_step_size = 20  # 15 epoch 마다 lr * p_lr_decay_factor
args.f_lr_decay_factor = 0.5

args.T_0 = 20        # 최초 주기값
args.T_mult = 2      # 최초 주기값에 비해 얼만큼 주기를 늘려갈 것인지
args.eta_max = 0.05  # lr 최대값
args.T_up = 10        # Warm up 시 필요한 epoch 수(일반적으로 짧은 수)
args.gamma = 0.5     # 주기가 반복될수록 곱해지는 scale 값

args.batch_lst = [32, 64]
args.cutoff_lst = [5.0, 8.0, 12.0]
#args.cutoff_lst = [8.0, 12.0]
args.num_layers_lst = [2, 6, 10]
args.z_dim_lst = [256, 512]
args.dropout_rate_lst = [0.0, 0.2, 0.5]

evaluator = Finetune(args=args, log_interval=10)
auc_m_lst, auc_sd_lst, paras, total = evaluator.grid_search(args)
#loss, sd = evaluator.evaluate()
```

    #Params: 367873
    ====FOLD 1====
    StepLR


    Finetune: epoch 150: 100%|██████████| 150/150 [07:39<00:00,  3.06s/it, best_test_loss=0.957, best_val_loss=0.804, train_rmse=0.722]


```python

```
